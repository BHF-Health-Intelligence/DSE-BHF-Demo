{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe058713-1bd6-47c3-a100-43c837fb85bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "#How to read data from the DSE into a notebook using R\n",
    "\n",
    "\n",
    "Firstly make sure that the default language is R. You can change the default language in the dropdown above, next to Help. This sets the language for the notebook, but you can also change the language for each cell with the drop-down in the top right of the cell (_this_ cell is markdown)\n",
    "\n",
    "\n",
    "\n",
    "###Data\n",
    "A reminder: data in the DSE is stored in the catalog, which you can access from the menu on the left. Data is generally stored hierarchically in this order:\n",
    "\n",
    "* Catalog (usually devcatalog), within which there are:\n",
    "  * Schema - separated into the medallion architecture, and contain:\n",
    "    * Tables\n",
    "    * Views\n",
    "    * Volumes\n",
    "\n",
    "Tables and views are the way most structured data is stored - these are very similar to SQL tables you may be familiar with. They are essentially dataframes with defined types for each column.  Volumes are how we can store unstructured data and can contain basically anything - csv files, zip archives, etc. You can access both tables and volumes from within a notebook. \n",
    "\n",
    "\n",
    "The medallion architecture is a method of organizing schema:\n",
    "\n",
    " * ðŸ¥‰ Bronze contains raw and unprocessed data - this may be a table or a store of csv files. \n",
    " * ðŸ¥ˆ Silver layer data builds on the bronze layer - cleaned or reformatted bronze-level data for example. \n",
    " * ðŸ¥‡ Gold layer data is where the finished data output lives - this is reserved for clean and easily digestible data that can be piped to dashboards etc. \n",
    "\n",
    "You can add data to the DSE by going into the catalog and creating schema as needed - you can upload as a csv in the first instance, or scrape data using a notebook like this one. You could also create a volume and upload raw files, then process them yourself into a silver layer. You can change the permissions on the schema/tables to decide who can have access to the data. \n",
    "\n",
    "\n",
    "###Clusters\n",
    "\n",
    "Computational operations using R in the DSE are performed on clusters - this is basically a virtual computer. You can choose/create a cluster using the dropdown above right - it will take a little while to spin up the first time it's created or switched on. The default settings are probably fine for your cluster, unless you have a need for large processing power or RAM. Remember that clusters are shared, so there _may_ be multiple users on a specific cluster, be mindful of this before you go restarting them willy-nilly. For a personal cluster, this is not much of a problem as you are the only user. Also, the cluster costs money all the time that it's enabled - so best practice is to turn it off if you're done with it and you know nobody else is going to use it. You don't need to be too parsimonious though - the cluster will turn itself off if it's idle for too long.\n",
    "\n",
    "The DSE uses [Apache Spark](https://www.databricks.com/glossary/what-is-apache-spark) under-the-hood, which is a powerful technology that allows us to access and efficiently work with large datasets across clusters as if we were working with just one machine. \n",
    "\n",
    "To work with Spark in R, we use the SparkR library, which (should) come pre-installed on the cluster. Libraries are specficic to clusters in the DSE, so different clusters may have different libraries (or versions or libraries) installed - this is useful to keep in mind. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2528cf5-3e70-4e61-b0ee-9df70aa1548f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "###Reading Data\n",
    "\n",
    "As an example, we will read in data from the listsizes table, in the schema epd_silver, in the devcatalog. This is a table containing data on the list sizes (ie number of patients served) by GP practices. The data has already been converted from its raw monthly form, in the bronze layer, to a combined dataset in the silver layer \n",
    "\n",
    "The notation for specifying the table is very simple:\n",
    "\n",
    "\\<catalog\\>.\\<schema\\>.\\<table\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4579143d-0460-4c97-b2bd-5cfa5ed96bfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Load the library\n",
    "library(SparkR)\n",
    "\n",
    "# Set the catalog, schema, and table name\n",
    "catalog <- \"devcatalog\"\n",
    "schema <- \"epd_silver\"\n",
    "table <- \"listsizes\"\n",
    "\n",
    "# Read the table using SparkR\n",
    "data_tbl <- tableToDF(paste(catalog, schema, table, sep = \".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50880bfb-5a3e-4d0b-8bd3-e09d04f59dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Easy! \n",
    "\n",
    "The data_tbl object that is returned is not a regular dataframe that you might see in RStudio - it's a Spark dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15fd9029-7494-4d66-8599-945283f6e6e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class(data_tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89aa3cde-a3d7-4074-a145-4a97a62a95d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is a specific type of dataframe that is optimized for large datasets - it uses so-called [lazy-loading](https://en.wikipedia.org/wiki/Lazy_loading), meaning the table is not saved in memory and operations are not performed on the table until the user specifically requests them. This is good for large datasets, but it means that the familiar tidyverse functions you're used to may not work. A lot of them do however:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41486251-69f0-4de9-ad98-0eccd7e02c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse) #You will need to install this on your cluster first\n",
    "\n",
    "data_tbl %>% head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c717c6e2-dfe5-45b9-a3ea-b13d3a03933c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "...is fine, but you may see messages about how functions in Spark are being masked - this means that tidyverse is taking precedence in your notebook's namespace (ie that a function like slice() is now being handled by tidyverse and not SparkR)  \n",
    "\n",
    " \n",
    "There are Spark functions specifically suited to Spark dataframes that may work better. A useful one is display()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c798ab-165e-447a-87c1-f58d9ad727ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(data_tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31bb9953-4abd-49e0-b56e-4ab4120bb8cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "which looks prettier and has a download button. You can even click the little + to make a plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb395a53-fffc-412d-a810-422cf63b48f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can use the collect() function in SparkR to convert the table from a Spark dataframe to a regular dataframe. This is generally fine for datasets that are not very large (so for anything with less than around a million rows or so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c2bc04d-b363-43af-b0dd-644e70a3ac1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "regular_dataframe=SparkR::collect(data_tbl)\n",
    "class(regular_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9a9d57b-dc82-4658-88fb-02c45b1d89f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can still use the display() function, which is handy, but now you can also use the familiar tidyverse operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4af0787-1d1e-406b-869f-5c93188f8d7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "regular_dataframe %>% \n",
    "  group_by(YEAR_MONTH) %>%\n",
    "  summarize(total_patients=sum(total_patients)) %>% \n",
    "  display() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "808abc40-fa3d-4ea5-9d4c-25b707be83a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Saving data\n",
    "\n",
    "Saving data to the catalog in the DSE is easy. The command to save a dataframe df is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "382bac5a-155e-4847-beef-b2c425577ee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "saveAsTable(df, \"<catalog>.<schema>.<table>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ce829fe-8f47-4912-b6d7-67fdf98fc21c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "which follows the same conventions as the reading data. There are some subtleties here in that this command only works for Spark dataframes, and not the tidyverse-compatible types with which we're all familiar. \n",
    "\n",
    "###BHFDSE \n",
    "Fortunately we have a github package designed specifically for using the DSE. You can install it with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3a1f31a-2be7-4fc2-beb7-d3ba41d9c5da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "library(devtools)\n",
    "install_github(\"BHF-Health-Intelligence/DSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91b44eef-0dfb-4f59-8c90-7bae6092eded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "And can read data with it using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee4ae09-424d-4cec-93d2-480d4cc5e8e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "library(BHFDSE)\n",
    "data=read_data(\"epd_silver.listsizes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3a4b529-cf75-434a-a692-6b13867d1067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note how the catalog name is now not necessary - it defaults to devcatalog. \n",
    "\n",
    "The BHFDSE library  also has a routine called save_data which does exactly what it says it does, but this function can accept Spark dataframes or the normal data.frame class we use in R. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0293861-4cbd-4471-9ae4-74775d7f9d8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_data(data,\"epd_silver.listsizes_testagain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f580571a-2c35-419c-ae58-cfcffe32a6bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There is also a \"mode\" argument for save_data - this lets you append or overwrite the data in the table - so be careful when specifying this! You can get more info in the usual R way, by running ?save_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "798103f5-155e-4610-912c-4bfd285fa15e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "?save_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3961e1b3-007c-4980-99eb-b87d4463f056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "r",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Working with Data - R",
   "widgets": {}
  },
  "language_info": {
   "name": "r"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
