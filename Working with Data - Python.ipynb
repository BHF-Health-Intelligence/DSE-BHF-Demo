{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe058713-1bd6-47c3-a100-43c837fb85bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "#How to read data from the DSE into a notebook using Python\n",
    "\n",
    "\n",
    "Firstly make sure that the default language is Python. You can change the default language in the dropdown above, next to Help. This sets the language for the notebook, but you can also change the language for each cell with the drop-down in the top right of the cell (_this_ cell is markdown)\n",
    "\n",
    "\n",
    "###Data\n",
    "A reminder: data in the DSE is stored in the catalog, which you can access from the menu on the left. Data is generally stored hierarchically in this order:\n",
    "\n",
    "* Catalog (usually devcatalog), within which there are:\n",
    "  * Schema - separated into the medallion architecture, and contain:\n",
    "    * Tables\n",
    "    * Views\n",
    "    * Volumes\n",
    "\n",
    "Tables and views are the way most structured data is stored - these are very similar to SQL tables you may be familiar with. They are essentially dataframes with defined types for each column.  Volumes are how we can store unstructured data and can contain basically anything - csv files, zip archives, etc. You can access both tables and volumes from within a notebook. \n",
    "\n",
    "\n",
    "The medallion architecture is a method of organizing schema:\n",
    "\n",
    " * ðŸ¥‰ Bronze contains raw and unprocessed data - this may be a table or a store of csv files. \n",
    " * ðŸ¥ˆ Silver layer data builds on the bronze layer - cleaned or reformatted bronze-level data for example. \n",
    " * ðŸ¥‡ Gold layer data is where the finished data output lives - this is reserved for clean and easily digestible data that can be piped to dashboards etc. \n",
    "\n",
    "You can add data to the DSE by going into the catalog and creating schema as needed - you can upload as a csv in the first instance, or scrape data using a notebook like this one. You could also create a volume and upload raw files, then process them yourself into a silver layer. You can change the permissions on the schema/tables to decide who can have access to the data. \n",
    "\n",
    "\n",
    "###Clusters\n",
    "**For Python, you can use \"serverless\" when selecting the cluster, which will automatically provision resources as needed - this is the recommended practice, so you don't have to worry about clusters**  \n",
    "\n",
    "Computational operations in the DSE are performed (generally speaking) on clusters - this is basically a virtual computer. You can choose/create a cluster using the dropdown above right - it will take a little while to spin up the first time it's created or switched on. The default settings are probably fine for your cluster, unless you have a need for large processing power or RAM. Remember that clusters are shared, so there _may_ be multiple users on a specific cluster, be mindful of this before you go restarting them willy-nilly. For a personal cluster, this is not much of a problem as you are the only user. Also, the cluster costs money all the time that it's enabled - so best practice is to turn it off if you're done with it and you know nobody else is going to use it. You don't need to be too parsimonious though - the cluster will turn itself off if it's idle for too long.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The DSE uses [Apache Spark](https://www.databricks.com/glossary/what-is-apache-spark) under-the-hood, which is a powerful technology that allows us to access and efficiently work with large datasets across clusters.  \n",
    "\n",
    "To work with Spark in Python, we use PySpark - the Python API for Spark - which lets us interface directly with Spark. This is very powerful but it requires learning a new syntax - PySpark code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2528cf5-3e70-4e61-b0ee-9df70aa1548f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "###Reading Data\n",
    "\n",
    "As an example, we will read in data from the listsizes table, in the schema epd_silver, in the devcatalog. This is a table containing data on the list sizes (ie number of patients served) by GP practices. The data has already been converted from its raw monthly form, in the bronze layer, to a combined dataset in the silver layer \n",
    "\n",
    "The notation for specifying the table is very simple:\n",
    "\n",
    "\\<catalog\\>.\\<schema\\>.\\<table\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4579143d-0460-4c97-b2bd-5cfa5ed96bfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "catalog = \"devcatalog\"\n",
    "schema = \"epd_silver\"\n",
    "table = \"listsizes\"\n",
    "\n",
    "data_tbl = spark.table(f\"{catalog}.{schema}.{table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50880bfb-5a3e-4d0b-8bd3-e09d04f59dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Easy! \n",
    "\n",
    "The data_tbl object that is returned is not a regular dataframe that you might be used to in pandas/polars - it's a Spark dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15fd9029-7494-4d66-8599-945283f6e6e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(data_tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89aa3cde-a3d7-4074-a145-4a97a62a95d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is a specific type of dataframe that is optimized for large datasets and clusters - it uses so-called [lazy-loading](https://en.wikipedia.org/wiki/Lazy_loading), meaning the table is not saved in memory and operations are not performed on the table until the user specifically requests them. This is good for large datasets, but it means that the familiar pandas functions you're used to may not work. A lot of them do however:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41486251-69f0-4de9-ad98-0eccd7e02c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_tbl.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c717c6e2-dfe5-45b9-a3ea-b13d3a03933c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "...is fine, but there are Spark functions specifically suited to Spark dataframes that may work better. A useful one is display()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c798ab-165e-447a-87c1-f58d9ad727ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(data_tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31bb9953-4abd-49e0-b56e-4ab4120bb8cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "which looks prettier and has a download button. You can even click the little + to make a plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb395a53-fffc-412d-a810-422cf63b48f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can use the collect() function in SparkR to convert the table from a Spark dataframe to a regular dataframe. This is generally fine for datasets that are not very large (so for anything with less than around a million rows or so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c2bc04d-b363-43af-b0dd-644e70a3ac1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "regular_dataframe = data_tbl.toPandas()\n",
    "type(regular_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9a9d57b-dc82-4658-88fb-02c45b1d89f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can still use the display() function, which is handy, but now you can also use the familiar pandas operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4af0787-1d1e-406b-869f-5c93188f8d7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(regular_dataframe.groupby(\"YEAR_MONTH\")[\"total_patients\"].sum().reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "808abc40-fa3d-4ea5-9d4c-25b707be83a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Saving data\n",
    "\n",
    "Saving data to the catalog in the DSE is easy. The command to save a dataframe df is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a8db5e2-9e6b-440d-ba39-6393a77fb99b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(\"<catalog>.<schema>.<table>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ce829fe-8f47-4912-b6d7-67fdf98fc21c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can specify different modes - append, overwrite, etc. Be careful when using overwrite! We _can_ restore dropped tables, but we'd really rather not\n",
    "\n",
    "There are some subtleties here in that this command only works for Spark dataframes, and not the pandas-compatible types with which we're all familiar. \n",
    " \n",
    "Fortunately it's very straightforward to change our pandas dataframe to Spark format before saving:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f0d05f1-9d2e-47b0-9ad3-dbf2c0d20863",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(regular_dataframe)\n",
    "spark_df.write.mode(\"overwrite\").saveAsTable(\"<catalog>.<schema>.<table>\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Working with Data - Python",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
